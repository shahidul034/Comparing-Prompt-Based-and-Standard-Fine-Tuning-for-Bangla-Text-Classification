{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sdm/anaconda3/envs/openprompt/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# from datasets import load_dataset\n",
    "# raw_dataset = load_dataset('super_glue', 'cb', cache_dir=\"../datasets/.cache/huggingface_datasets\")\n",
    "from datasets import load_from_disk, Dataset,concatenate_datasets\n",
    "import pandas as pd\n",
    "df=pd.read_csv(\"data/Train.csv\")\n",
    "dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['Data', 'Label'],\n",
       "         num_rows: 10060\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['Data', 'Label'],\n",
       "         num_rows: 2515\n",
       "     })\n",
       " }),\n",
       " DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['Data', 'Label'],\n",
       "         num_rows: 1257\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['Data', 'Label'],\n",
       "         num_rows: 1258\n",
       "     })\n",
       " }))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset=dataset.train_test_split(test_size=0.2)\n",
    "raw_dataset2=raw_dataset['test'].train_test_split(test_size=0.5)\n",
    "raw_dataset,raw_dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Data', 'Label'],\n",
      "        num_rows: 10060\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['Data', 'Label'],\n",
      "        num_rows: 1257\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['Data', 'Label'],\n",
      "        num_rows: 1257\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets.dataset_dict import DatasetDict\n",
    "from datasets import Dataset\n",
    "d = {'train':Dataset.from_dict({'Data':raw_dataset['train']['Data'],'Label':raw_dataset['train']['Label']}),'validation':Dataset.from_dict({'Data':raw_dataset2['train']['Data'],'Label':raw_dataset2['train']['Label']}),'test':Dataset.from_dict({'Data':raw_dataset2['train']['Data'],'Label':raw_dataset2['train']['Label']})}\n",
    "raw_datasets=DatasetDict(d)\n",
    "print(raw_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"guid\": 0,\n",
      "  \"label\": 0,\n",
      "  \"meta\": {},\n",
      "  \"text_a\": \"\\u09a6\\u09be\\u09a6\\u09be \\u098f\\u09ac\\u09be\\u09b0 \\u098f\\u0995\\u09ac\\u09be\\u09b0 \\u09ac\\u09be\\u09b2\\u09cd\\u09b2\\u09be\\u09b0\\u09be\\u09ae \\u09ae\\u09c1\\u09b2\\u09cd\\u09b2\\u09bf\\u0995 \\u098f\\u09a8\\u09cd\\u09a1 \\u09b0\\u09be\\u09a7\\u09be\\u09b0\\u09ae\\u09a3 \\u09ae\\u09c1\\u09b2\\u09cd\\u09b2\\u09bf\\u0995 \\u098f\\u09b0 \\u09a6\\u09cb\\u0995\\u09be\\u09a8 \\u098f\\u09b0 \\u09b0\\u09b8\\u0997\\u09cb\\u09b2\\u09cd\\u09b2\\u09be \\u09a8\\u09be \\u0997\\u09c7\\u09b2\\u09c7 \\u09ac\\u09bf\\u09b0\\u09be\\u099f \\u098f\\u0995\\u099f\\u09be \\u099c\\u09bf\\u09a8\\u09bf\\u09b8 \\u0995\\u09bf\\u09a8\\u09cd\\u09a4\\u09c1 \\u09ae\\u09bf\\u09b8 \\u0995\\u09b0\\u09c7\\u09af\\u09be\\u09ac\\u09c7\\u09a8\",\n",
      "  \"text_b\": \"\",\n",
      "  \"tgt_text\": null\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from openprompt.data_utils import InputExample\n",
    "\n",
    "dataset = {}\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    dataset[split] = []\n",
    "    for idx,data in enumerate(raw_datasets[split]):\n",
    "        input_example = InputExample(text_a = data['Data'], label=int(data['Label']), guid=idx)\n",
    "        dataset[split].append(input_example)\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sdm/anaconda3/envs/openprompt/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You are using a model of type mt5 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "from openprompt.plms import load_plm\n",
    "plm, tokenizer, model_config, WrapperClass = load_plm(\"t5\", \"google/mt5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openprompt.prompts import ManualTemplate\n",
    "# template_text = '{\"placeholder\":\"text_a\"}. এটা হল {\"mask\"}.'\n",
    "template_text = '{\"placeholder\":\"text_a\"}. sentiment: {\"mask\"}.'\n",
    "# template_text = '[CLS] {\"placeholder\":\"text_a\"}. Sentiment: {\"mask\"}. [SEP]'\n",
    "mytemplate = ManualTemplate(tokenizer=tokenizer, text=template_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'text': 'দাদা এবার একবার বাল্লারাম মুল্লিক এন্ড রাধারমণ মুল্লিক এর দোকান এর রসগোল্লা না গেলে বিরাট একটা জিনিস কিন্তু মিস করেযাবেন', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '. sentiment:', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': '.', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'label': 0}]\n"
     ]
    }
   ],
   "source": [
    "wrapped_example = mytemplate.wrap_one_example(dataset['train'][0])\n",
    "print(wrapped_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Additional special tokens: ['<mask>']\n",
      "Additional special tokens ids: [250100]\n"
     ]
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({'additional_special_tokens': ['<mask>']})\n",
    "print(\"Additional special tokens:\", tokenizer.additional_special_tokens) \n",
    "print(\"Additional special tokens ids:\", tokenizer.additional_special_tokens_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapped_t5tokenizer = WrapperClass(max_seq_length=128, decoder_max_length=3, tokenizer=tokenizer,truncate_method=\"head\")\n",
    "# or\n",
    "from openprompt.plms import T5TokenizerWrapper\n",
    "wrapped_t5tokenizer= T5TokenizerWrapper(max_seq_length=128, decoder_max_length=3, tokenizer=tokenizer,truncate_method=\"head\")\n",
    "# tokenized_example = wrapped_t5tokenizer.tokenize_one_example(wrapped_example, teacher_forcing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 10060it [00:01, 6265.86it/s]\n"
     ]
    }
   ],
   "source": [
    "from openprompt import PromptDataLoader\n",
    "\n",
    "train_dataloader = PromptDataLoader(dataset=dataset[\"train\"], template=mytemplate, tokenizer=tokenizer,\n",
    "    tokenizer_wrapper_class=WrapperClass, max_seq_length=256, decoder_max_length=3,\n",
    "    batch_size=4,shuffle=True, teacher_forcing=False, predict_eos_token=False,\n",
    "    truncate_method=\"head\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_example = wrapped_t5tokenizer.tokenize_one_example(wrapped_example, teacher_forcing=False)\n",
    "# print(tokenized_example)\n",
    "# print(tokenizer.convert_ids_to_tokens(tokenized_example['input_ids']))\n",
    "# print(tokenizer.convert_ids_to_tokens(tokenized_example['decoder_input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[156989,      0]],\n",
      "\n",
      "        [[   259, 137088]],\n",
      "\n",
      "        [[153418,    265]]])\n",
      "tensor([[-1.4308, -0.8622, -1.0828],\n",
      "        [-2.1807, -0.2454, -2.2571]])\n"
     ]
    }
   ],
   "source": [
    "from openprompt.prompts import ManualVerbalizer\n",
    "import torch\n",
    "\n",
    "# for example the verbalizer contains multiple label words in each class\n",
    "myverbalizer = ManualVerbalizer(tokenizer, num_classes=3,\n",
    "                        label_words=[[\"Neutral\"], [\"Positive\"], [\"Negative\"]])\n",
    "\n",
    "print(myverbalizer.label_words_ids)\n",
    "logits = torch.randn(2,len(tokenizer)) # creating a pseudo output from the plm, and\n",
    "print(myverbalizer.process_logits(logits)) # see what the verbalizer do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sdm/anaconda3/envs/openprompt/lib/python3.8/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from openprompt import PromptForClassification\n",
    "\n",
    "use_cuda = True\n",
    "prompt_model = PromptForClassification(plm=plm,template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\n",
    "if use_cuda:\n",
    "    prompt_model=  prompt_model.cuda()\n",
    "\n",
    "# Now the training is standard\n",
    "from transformers import  AdamW, get_linear_schedule_with_warmup\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "# it's always good practice to set no decay to biase and LayerNorm parameters\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, average loss: 11.264082431793213\n",
      "Epoch 0, average loss: 1.7002377524679781\n",
      "Epoch 0, average loss: 1.3424039277994986\n",
      "Epoch 0, average loss: 1.201624226343158\n",
      "Epoch 0, average loss: 1.1289077883780891\n",
      "Epoch 0, average loss: 1.080813111626057\n",
      "Epoch 0, average loss: 1.0393867160369985\n",
      "Epoch 0, average loss: 1.0199579295630639\n",
      "Epoch 0, average loss: 0.9956281465429468\n",
      "Epoch 0, average loss: 0.9776344262476506\n",
      "Epoch 0, average loss: 0.9663131166390793\n",
      "Epoch 0, average loss: 0.952214144941034\n",
      "Epoch 0, average loss: 0.9401341166700579\n",
      "Epoch 0, average loss: 0.9344097701131656\n",
      "Epoch 0, average loss: 0.9269766013075553\n",
      "Epoch 0, average loss: 0.9203895375866706\n",
      "Epoch 0, average loss: 0.910217711415965\n",
      "Epoch 0, average loss: 0.8997490011947941\n",
      "Epoch 0, average loss: 0.8914469151957153\n",
      "Epoch 0, average loss: 0.8828864262882937\n",
      "Epoch 0, average loss: 0.8752928335465245\n",
      "Epoch 0, average loss: 0.8705877384505847\n",
      "Epoch 0, average loss: 0.8648724835230669\n",
      "Epoch 0, average loss: 0.8612174421700962\n",
      "Epoch 0, average loss: 0.8567745500383007\n",
      "Epoch 0, average loss: 0.8540961240856744\n",
      "Epoch 1, average loss: 0.3242378756403923\n",
      "Epoch 1, average loss: 0.5227655103594503\n",
      "Epoch 1, average loss: 0.5483506758222448\n",
      "Epoch 1, average loss: 0.552697396157186\n",
      "Epoch 1, average loss: 0.5676337914362976\n",
      "Epoch 1, average loss: 0.5792244797548376\n",
      "Epoch 1, average loss: 0.5893311987397418\n",
      "Epoch 1, average loss: 0.596745085647128\n",
      "Epoch 1, average loss: 0.5943272619704458\n",
      "Epoch 1, average loss: 0.5958145773404808\n",
      "Epoch 1, average loss: 0.5985300136100828\n",
      "Epoch 1, average loss: 0.6024009330555654\n",
      "Epoch 1, average loss: 0.5987011586790889\n",
      "Epoch 1, average loss: 0.5942899824507083\n",
      "Epoch 1, average loss: 0.6013866953027651\n",
      "Epoch 1, average loss: 0.5983823416193453\n",
      "Epoch 1, average loss: 0.6020062557336167\n",
      "Epoch 1, average loss: 0.6017910095966463\n",
      "Epoch 1, average loss: 0.604023759757488\n",
      "Epoch 1, average loss: 0.606261319754798\n",
      "Epoch 1, average loss: 0.6079753388310657\n",
      "Epoch 1, average loss: 0.6074819364037797\n",
      "Epoch 1, average loss: 0.6087816310765224\n",
      "Epoch 1, average loss: 0.6091554264166189\n",
      "Epoch 1, average loss: 0.6101085170062428\n",
      "Epoch 1, average loss: 0.6076084069226855\n",
      "Epoch 2, average loss: 0.4921235367655754\n",
      "Epoch 2, average loss: 0.32873030755595833\n",
      "Epoch 2, average loss: 0.311669083985288\n",
      "Epoch 2, average loss: 0.32439986081633543\n",
      "Epoch 2, average loss: 0.32749197215122167\n",
      "Epoch 2, average loss: 0.32881217685099545\n",
      "Epoch 2, average loss: 0.35114233064244\n",
      "Epoch 2, average loss: 0.3605590701070707\n",
      "Epoch 2, average loss: 0.3651002586532983\n",
      "Epoch 2, average loss: 0.3631524271453047\n",
      "Epoch 2, average loss: 0.3763529130751676\n",
      "Epoch 2, average loss: 0.38694911379136393\n",
      "Epoch 2, average loss: 0.3847272888505088\n",
      "Epoch 2, average loss: 0.3836997469968324\n",
      "Epoch 2, average loss: 0.3874448166235806\n",
      "Epoch 2, average loss: 0.3935919231662313\n",
      "Epoch 2, average loss: 0.395621673948278\n",
      "Epoch 2, average loss: 0.4027353755301565\n",
      "Epoch 2, average loss: 0.4024861993909773\n",
      "Epoch 2, average loss: 0.40280479844157674\n",
      "Epoch 2, average loss: 0.40278126459117963\n",
      "Epoch 2, average loss: 0.40496608530927525\n",
      "Epoch 2, average loss: 0.4056594326558627\n",
      "Epoch 2, average loss: 0.40733574710446097\n",
      "Epoch 2, average loss: 0.4077367948304798\n",
      "Epoch 2, average loss: 0.4094289478967428\n",
      "Epoch 3, average loss: 0.0986323393881321\n",
      "Epoch 3, average loss: 0.1411218896882572\n",
      "Epoch 3, average loss: 0.14325487436022147\n",
      "Epoch 3, average loss: 0.13413664677490855\n",
      "Epoch 3, average loss: 0.12576979924643408\n",
      "Epoch 3, average loss: 0.16247576051936344\n",
      "Epoch 3, average loss: 0.1549627175475226\n",
      "Epoch 3, average loss: 0.16107512683128336\n",
      "Epoch 3, average loss: 0.1600677890201041\n",
      "Epoch 3, average loss: 0.16662241275973344\n",
      "Epoch 3, average loss: 0.17609696523501778\n",
      "Epoch 3, average loss: 0.1836947550220195\n",
      "Epoch 3, average loss: 0.18581010104970364\n",
      "Epoch 3, average loss: 0.1909616699369072\n",
      "Epoch 3, average loss: 0.19367824312783563\n",
      "Epoch 3, average loss: 0.19684778959843305\n",
      "Epoch 3, average loss: 0.20062007683505562\n",
      "Epoch 3, average loss: 0.20204190572510736\n",
      "Epoch 3, average loss: 0.20613114079520825\n",
      "Epoch 3, average loss: 0.20643104235248327\n",
      "Epoch 3, average loss: 0.21215060709171205\n",
      "Epoch 3, average loss: 0.21296634860089425\n",
      "Epoch 3, average loss: 0.21364019450553715\n",
      "Epoch 3, average loss: 0.21926549376762677\n",
      "Epoch 3, average loss: 0.21973613850228968\n",
      "Epoch 3, average loss: 0.22300641022090098\n",
      "Epoch 4, average loss: 0.06649158103391528\n",
      "Epoch 4, average loss: 0.14985480254165617\n",
      "Epoch 4, average loss: 0.1163569425976441\n",
      "Epoch 4, average loss: 0.10436185517697981\n",
      "Epoch 4, average loss: 0.10916443515553913\n",
      "Epoch 4, average loss: 0.11072151097633683\n",
      "Epoch 4, average loss: 0.1168442051470948\n",
      "Epoch 4, average loss: 0.11561294149238358\n",
      "Epoch 4, average loss: 0.1202261231373678\n",
      "Epoch 4, average loss: 0.11957754482064611\n",
      "Epoch 4, average loss: 0.12766197175811456\n",
      "Epoch 4, average loss: 0.13364278683489936\n",
      "Epoch 4, average loss: 0.13713304721537548\n",
      "Epoch 4, average loss: 0.13542597817712257\n",
      "Epoch 4, average loss: 0.1356698044374375\n",
      "Epoch 4, average loss: 0.14083506559600525\n",
      "Epoch 4, average loss: 0.1398124103035637\n",
      "Epoch 4, average loss: 0.1412971576622152\n",
      "Epoch 4, average loss: 0.14551692797346522\n",
      "Epoch 4, average loss: 0.14782735181458714\n",
      "Epoch 4, average loss: 0.15056507315106815\n",
      "Epoch 4, average loss: 0.15125711779405637\n",
      "Epoch 4, average loss: 0.1509693993218069\n",
      "Epoch 4, average loss: 0.15163224905757242\n",
      "Epoch 4, average loss: 0.15496756944626186\n",
      "Epoch 4, average loss: 0.15728418428409813\n",
      "Epoch 5, average loss: 0.0037706190487369895\n",
      "Epoch 5, average loss: 0.06954557253601368\n",
      "Epoch 5, average loss: 0.06653248503939137\n",
      "Epoch 5, average loss: 0.08046587136936827\n",
      "Epoch 5, average loss: 0.09215997446939292\n",
      "Epoch 5, average loss: 0.09023739364152042\n",
      "Epoch 5, average loss: 0.08953760095493296\n",
      "Epoch 5, average loss: 0.09719544187906168\n",
      "Epoch 5, average loss: 0.09872232158466118\n",
      "Epoch 5, average loss: 0.09571779406633746\n",
      "Epoch 5, average loss: 0.09945540474841245\n",
      "Epoch 5, average loss: 0.0970824740991136\n",
      "Epoch 5, average loss: 0.10342791559005203\n",
      "Epoch 5, average loss: 0.1093863381538121\n",
      "Epoch 5, average loss: 0.11076130932389876\n",
      "Epoch 5, average loss: 0.11280657614308084\n",
      "Epoch 5, average loss: 0.11101014576068453\n",
      "Epoch 5, average loss: 0.1145192672105212\n",
      "Epoch 5, average loss: 0.11734529815434594\n",
      "Epoch 5, average loss: 0.11886244917806814\n",
      "Epoch 5, average loss: 0.12213144771820654\n",
      "Epoch 5, average loss: 0.12299809284404953\n",
      "Epoch 5, average loss: 0.1253421467200504\n",
      "Epoch 5, average loss: 0.12936460100739902\n",
      "Epoch 5, average loss: 0.1325892595348184\n",
      "Epoch 5, average loss: 0.13318924218623257\n",
      "Epoch 6, average loss: 0.2206969214603305\n",
      "Epoch 6, average loss: 0.08596337866601614\n",
      "Epoch 6, average loss: 0.074101747060526\n",
      "Epoch 6, average loss: 0.07452903501156988\n",
      "Epoch 6, average loss: 0.07414414151381647\n",
      "Epoch 6, average loss: 0.07265162263157902\n",
      "Epoch 6, average loss: 0.08063630543637618\n",
      "Epoch 6, average loss: 0.0815958341864874\n",
      "Epoch 6, average loss: 0.08204938932835729\n",
      "Epoch 6, average loss: 0.08266709925440766\n",
      "Epoch 6, average loss: 0.08406281274559961\n",
      "Epoch 6, average loss: 0.08543180816584725\n",
      "Epoch 6, average loss: 0.08833737469677345\n",
      "Epoch 6, average loss: 0.09010201803556714\n",
      "Epoch 6, average loss: 0.09162891691805151\n",
      "Epoch 6, average loss: 0.09323740166935982\n",
      "Epoch 6, average loss: 0.09513944681408104\n",
      "Epoch 6, average loss: 0.09848956374506165\n",
      "Epoch 6, average loss: 0.09946686732456347\n",
      "Epoch 6, average loss: 0.1025955810262346\n",
      "Epoch 6, average loss: 0.10516027575564661\n",
      "Epoch 6, average loss: 0.10471526316792798\n",
      "Epoch 6, average loss: 0.11016515340863157\n",
      "Epoch 6, average loss: 0.11126412509866002\n",
      "Epoch 6, average loss: 0.11296500646142202\n",
      "Epoch 6, average loss: 0.11490058235788657\n",
      "Epoch 7, average loss: 0.18427374865859747\n",
      "Epoch 7, average loss: 0.07039681531936295\n",
      "Epoch 7, average loss: 0.06892333629546574\n",
      "Epoch 7, average loss: 0.0837079466090418\n",
      "Epoch 7, average loss: 0.09399016579453164\n",
      "Epoch 7, average loss: 0.09185222455557977\n",
      "Epoch 7, average loss: 0.09703718270895745\n",
      "Epoch 7, average loss: 0.09688474569281717\n",
      "Epoch 7, average loss: 0.09205479373789481\n",
      "Epoch 7, average loss: 0.09197865406674872\n",
      "Epoch 7, average loss: 0.09528713178178351\n",
      "Epoch 7, average loss: 0.09552455757627158\n",
      "Epoch 7, average loss: 0.09589079185492946\n",
      "Epoch 7, average loss: 0.0932323235694329\n",
      "Epoch 7, average loss: 0.09155698300418642\n",
      "Epoch 7, average loss: 0.0942024420106456\n",
      "Epoch 7, average loss: 0.09598961961308375\n",
      "Epoch 7, average loss: 0.09621104038334294\n",
      "Epoch 7, average loss: 0.0963168009092231\n",
      "Epoch 7, average loss: 0.09852368927463302\n",
      "Epoch 7, average loss: 0.10046960621851364\n",
      "Epoch 7, average loss: 0.10157068705720027\n",
      "Epoch 7, average loss: 0.10559686758205218\n",
      "Epoch 7, average loss: 0.10507346982415747\n",
      "Epoch 7, average loss: 0.10602084636906932\n",
      "Epoch 7, average loss: 0.10629889821828417\n",
      "Epoch 8, average loss: 0.027922372333705425\n",
      "Epoch 8, average loss: 0.04026660858378854\n",
      "Epoch 8, average loss: 0.042258522162687724\n",
      "Epoch 8, average loss: 0.042745319244300806\n",
      "Epoch 8, average loss: 0.05134586615622086\n",
      "Epoch 8, average loss: 0.05316928481408243\n",
      "Epoch 8, average loss: 0.061617527056141534\n",
      "Epoch 8, average loss: 0.06646119269096075\n",
      "Epoch 8, average loss: 0.06845711251610867\n",
      "Epoch 8, average loss: 0.07149039085728905\n",
      "Epoch 8, average loss: 0.07405985448080347\n",
      "Epoch 8, average loss: 0.07345388576514349\n",
      "Epoch 8, average loss: 0.07227772655219973\n",
      "Epoch 8, average loss: 0.07806171222798007\n",
      "Epoch 8, average loss: 0.07995760735837694\n",
      "Epoch 8, average loss: 0.08264879268340028\n",
      "Epoch 8, average loss: 0.08463383828961839\n",
      "Epoch 8, average loss: 0.08760734827392533\n",
      "Epoch 8, average loss: 0.08854192581009164\n",
      "Epoch 8, average loss: 0.09156439592719239\n",
      "Epoch 8, average loss: 0.09194243978050258\n",
      "Epoch 8, average loss: 0.09500151917508197\n",
      "Epoch 8, average loss: 0.09656484088958146\n",
      "Epoch 8, average loss: 0.09674801860553281\n",
      "Epoch 8, average loss: 0.0968261036846165\n",
      "Epoch 8, average loss: 0.095985594087545\n",
      "Epoch 9, average loss: 0.07012990192743018\n",
      "Epoch 9, average loss: 0.04650750693619164\n",
      "Epoch 9, average loss: 0.03684797994912294\n",
      "Epoch 9, average loss: 0.036527868291806126\n",
      "Epoch 9, average loss: 0.037999121046481156\n",
      "Epoch 9, average loss: 0.04032010184898555\n",
      "Epoch 9, average loss: 0.04541676832250088\n",
      "Epoch 9, average loss: 0.04793189423788199\n",
      "Epoch 9, average loss: 0.05176517868079075\n",
      "Epoch 9, average loss: 0.052501155094802056\n",
      "Epoch 9, average loss: 0.056373556151668315\n",
      "Epoch 9, average loss: 0.0558278977090161\n",
      "Epoch 9, average loss: 0.05740302511047429\n",
      "Epoch 9, average loss: 0.05811778972955061\n",
      "Epoch 9, average loss: 0.05685553843725662\n",
      "Epoch 9, average loss: 0.056489145166328546\n",
      "Epoch 9, average loss: 0.057560924485517986\n",
      "Epoch 9, average loss: 0.061359862867576886\n",
      "Epoch 9, average loss: 0.06266392505503458\n",
      "Epoch 9, average loss: 0.06543373139782328\n",
      "Epoch 9, average loss: 0.06787412385423719\n",
      "Epoch 9, average loss: 0.06890521842793362\n",
      "Epoch 9, average loss: 0.07038495253597767\n",
      "Epoch 9, average loss: 0.07080290544530612\n",
      "Epoch 9, average loss: 0.07236442287618072\n",
      "Epoch 9, average loss: 0.07245735785378662\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    tot_loss = 0\n",
    "    for step, inputs in enumerate(train_dataloader):\n",
    "        if use_cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        logits = prompt_model(inputs)\n",
    "        labels = inputs['label']\n",
    "        loss = loss_func(logits, labels)\n",
    "        loss.backward()\n",
    "        tot_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if step %100 ==1:\n",
    "            print(\"Epoch {}, average loss: {}\".format(epoch, tot_loss/(step+1)), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 1257it [00:00, 6506.77it/s]\n"
     ]
    }
   ],
   "source": [
    "validation_dataloader = PromptDataLoader(dataset=dataset[\"validation\"], template=mytemplate, tokenizer=tokenizer,\n",
    "    tokenizer_wrapper_class=WrapperClass, max_seq_length=256, decoder_max_length=3,\n",
    "    batch_size=4,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
    "    truncate_method=\"head\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7024661893396977\n"
     ]
    }
   ],
   "source": [
    "allpreds = []\n",
    "alllabels = []\n",
    "for step, inputs in enumerate(validation_dataloader):\n",
    "    if use_cuda:\n",
    "        inputs = inputs.cuda()\n",
    "    logits = prompt_model(inputs)\n",
    "    labels = inputs['label']\n",
    "    alllabels.extend(labels.cpu().tolist())\n",
    "    allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
    "\n",
    "acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from .utils import TokenizerWrapper\n",
    "from transformers.tokenization_utils import PreTrainedTokenizer\n",
    "from .mlm import MLMTokenizerWrapper\n",
    "from .seq2seq import T5LMTokenizerWrapper, T5TokenizerWrapper\n",
    "from .lm import LMTokenizerWrapper\n",
    "from transformers import BertConfig, BertTokenizer, BertModel, BertForMaskedLM, \\\n",
    "                         RobertaConfig, RobertaTokenizer, RobertaModel, RobertaForMaskedLM, \\\n",
    "                         AlbertTokenizer, AlbertConfig, AlbertModel, AlbertForMaskedLM, \\\n",
    "                         T5Config, T5Tokenizer, T5ForConditionalGeneration, \\\n",
    "                         OpenAIGPTTokenizer, OpenAIGPTLMHeadModel, OpenAIGPTConfig, \\\n",
    "                         GPT2Config, GPT2Tokenizer, GPT2LMHeadModel, \\\n",
    "                         OPTConfig, OPTForCausalLM\n",
    "from collections import namedtuple\n",
    "from yacs.config import CfgNode\n",
    "\n",
    "from openprompt.utils.logging import logger\n",
    "\n",
    "\n",
    "ModelClass = namedtuple(\"ModelClass\", ('config', 'tokenizer', 'model','wrapper'))\n",
    "\n",
    "_MODEL_CLASSES = {\n",
    "    'bert': ModelClass(**{\n",
    "        'config': BertConfig,\n",
    "        'tokenizer': BertTokenizer,\n",
    "        'model':BertForMaskedLM,\n",
    "        'wrapper': MLMTokenizerWrapper,\n",
    "    }),\n",
    "    'roberta': ModelClass(**{\n",
    "        'config': RobertaConfig,\n",
    "        'tokenizer': RobertaTokenizer,\n",
    "        'model':RobertaForMaskedLM,\n",
    "        'wrapper': MLMTokenizerWrapper\n",
    "    }),\n",
    "    'albert': ModelClass(**{\n",
    "        'config': AlbertConfig,\n",
    "        'tokenizer': AlbertTokenizer,\n",
    "        'model': AlbertForMaskedLM,\n",
    "        'wrapper': MLMTokenizerWrapper\n",
    "    }),\n",
    "    'gpt': ModelClass(**{\n",
    "        'config': OpenAIGPTConfig,\n",
    "        'tokenizer': OpenAIGPTTokenizer,\n",
    "        'model': OpenAIGPTLMHeadModel,\n",
    "        'wrapper': LMTokenizerWrapper\n",
    "    }),\n",
    "    'gpt2': ModelClass(**{\n",
    "        'config': GPT2Config,\n",
    "        'tokenizer': GPT2Tokenizer,\n",
    "        'model': GPT2LMHeadModel,\n",
    "        'wrapper': LMTokenizerWrapper\n",
    "    }),\n",
    "    't5':ModelClass(**{\n",
    "        'config': T5Config,\n",
    "        'tokenizer': T5Tokenizer,\n",
    "        'model': T5ForConditionalGeneration,\n",
    "        'wrapper': T5TokenizerWrapper\n",
    "    }),\n",
    "    't5-lm':ModelClass(**{\n",
    "        'config': T5Config,\n",
    "        'tokenizer': T5Tokenizer,\n",
    "        'model': T5ForConditionalGeneration,\n",
    "        'wrapper': T5LMTokenizerWrapper,\n",
    "    }),\n",
    "    'opt': ModelClass(**{\n",
    "        'config': OPTConfig,\n",
    "        'tokenizer': GPT2Tokenizer,\n",
    "        'model': OPTForCausalLM,\n",
    "        'wrapper': LMTokenizerWrapper,\n",
    "    }),\n",
    "}\n",
    "\n",
    "\n",
    "def get_model_class(plm_type: str):\n",
    "    return _MODEL_CLASSES[plm_type]\n",
    "\n",
    "\n",
    "def load_plm(model_name, model_path, specials_to_add = None):\n",
    "    r\"\"\"A plm loader using a global config.\n",
    "    It will load the model, tokenizer, and config simulatenously.\n",
    "\n",
    "    Args:\n",
    "        config (:obj:`CfgNode`): The global config from the CfgNode.\n",
    "\n",
    "    Returns:\n",
    "        :obj:`PreTrainedModel`: The pretrained model.\n",
    "        :obj:`tokenizer`: The pretrained tokenizer.\n",
    "        :obj:`model_config`: The config of the pretrained model.\n",
    "        :obj:`wrapper`: The wrapper class of this plm.\n",
    "    \"\"\"\n",
    "    model_class = get_model_class(plm_type = model_name)\n",
    "    model_config = model_class.config.from_pretrained(model_path)\n",
    "    # you can change huggingface model_config here\n",
    "    # if 't5'  in model_name: # remove dropout according to PPT~\\ref{}\n",
    "    #     model_config.dropout_rate = 0.0\n",
    "    if 'gpt' in model_name: # add pad token for gpt\n",
    "        specials_to_add = [\"<pad>\"]\n",
    "        # model_config.attn_pdrop = 0.0\n",
    "        # model_config.resid_pdrop = 0.0\n",
    "        # model_config.embd_pdrop = 0.0\n",
    "    model = model_class.model.from_pretrained(model_path, config=model_config)\n",
    "    if model_name==\"roberta\":\n",
    "        from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "        tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
    "    else:    \n",
    "        tokenizer = model_class.tokenizer.from_pretrained(model_path)\n",
    "    wrapper = model_class.wrapper\n",
    "\n",
    "\n",
    "    model, tokenizer = add_special_tokens(model, tokenizer, specials_to_add=specials_to_add)\n",
    "\n",
    "    if 'opt' in model_name:\n",
    "        tokenizer.add_bos_token=False\n",
    "    return model, tokenizer, model_config, wrapper\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_plm_from_config(config: CfgNode):\n",
    "    r\"\"\"A plm loader using a global config.\n",
    "    It will load the model, tokenizer, and config simulatenously.\n",
    "\n",
    "    Args:\n",
    "        config (:obj:`CfgNode`): The global config from the CfgNode.\n",
    "\n",
    "    Returns:\n",
    "        :obj:`PreTrainedModel`: The pretrained model.\n",
    "        :obj:`tokenizer`: The pretrained tokenizer.\n",
    "        :obj:`model_config`: The config of the pretrained model.\n",
    "        :obj:`model_config`: The wrapper class of this plm.\n",
    "    \"\"\"\n",
    "    plm_config = config.plm\n",
    "    model_class = get_model_class(plm_type = plm_config.model_name)\n",
    "    model_config = model_class.config.from_pretrained(plm_config.model_path)\n",
    "    # you can change huggingface model_config here\n",
    "    # if 't5'  in plm_config.model_name: # remove dropout according to PPT~\\ref{}\n",
    "    #     model_config.dropout_rate = 0.0\n",
    "    if 'gpt' in plm_config.model_name: # add pad token for gpt\n",
    "        if \"<pad>\" not in config.plm.specials_to_add:\n",
    "            config.plm.specials_to_add.append(\"<pad>\")\n",
    "    model = model_class.model.from_pretrained(plm_config.model_path, config=model_config)\n",
    "    tokenizer = model_class.tokenizer.from_pretrained(plm_config.model_path)\n",
    "    wrapper = model_class.wrapper\n",
    "    model, tokenizer = add_special_tokens(model, tokenizer, specials_to_add=config.plm.specials_to_add)\n",
    "    return model, tokenizer, model_config, wrapper\n",
    "\n",
    "def add_special_tokens(model: PreTrainedModel,\n",
    "                       tokenizer: PreTrainedTokenizer,\n",
    "                       specials_to_add: Optional[List[str]] = None):\n",
    "    r\"\"\"add the special_tokens to tokenizer if the special token\n",
    "    is not in the tokenizer.\n",
    "\n",
    "    Args:\n",
    "        model (:obj:`PreTrainedModel`): The pretrained model to resize embedding\n",
    "                after adding special tokens.\n",
    "        tokenizer (:obj:`PreTrainedTokenizer`): The pretrained tokenizer to add special tokens.\n",
    "        specials_to_add: (:obj:`List[str]`, optional): The special tokens to be added. Defaults to pad token.\n",
    "\n",
    "    Returns:\n",
    "        The resized model, The tokenizer with the added special tokens.\n",
    "\n",
    "    \"\"\"\n",
    "    if specials_to_add is None:\n",
    "        return model, tokenizer\n",
    "    for token in specials_to_add:\n",
    "        if \"pad\" in token.lower():\n",
    "            if tokenizer.pad_token is None:\n",
    "                tokenizer.add_special_tokens({'pad_token': token})\n",
    "                model.resize_token_embeddings(len(tokenizer))\n",
    "                logger.info(\"pad token is None, set to id {}\".format(tokenizer.pad_token_id))\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openprompt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
