{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, Dataset,concatenate_datasets\n",
    "import pandas as pd\n",
    "from datasets.dataset_dict import DatasetDict\n",
    "from datasets import Dataset\n",
    "from openprompt.data_utils import InputExample\n",
    "from openprompt.plms import load_plm\n",
    "from openprompt.prompts import ManualTemplate\n",
    "from openprompt.plms import MLMTokenizerWrapper\n",
    "from openprompt import PromptDataLoader\n",
    "from openprompt.prompts import ManualVerbalizer\n",
    "import torch\n",
    "from openprompt import PromptForClassification\n",
    "from transformers import  AdamW, get_linear_schedule_with_warmup\n",
    "import os\n",
    "\n",
    "def bert_model(model_root_name,model_full_name,df,epochs,column_name,class_name): \n",
    "\n",
    "    dataset_path=r\"data/Train.csv\"\n",
    "    # model_root_name=\"roberta\"\n",
    "    # model_full_name=\"google/mt5-base\"\n",
    "    # n_samples = 8\n",
    "\n",
    "    df=pd.read_csv(dataset_path)\n",
    "\n",
    "\n",
    "    df_neutral = df[df['Label'] == 0].sample(n=n_samples, random_state=1)\n",
    "    df_positive = df[df['Label'] == 1].sample(n=n_samples, random_state=1)\n",
    "    df_negative = df[df['Label'] == 2].sample(n=n_samples, random_state=1)\n",
    "\n",
    "\n",
    "    df_sampled = pd.concat([df_neutral, df_positive, df_negative], ignore_index=True)\n",
    "\n",
    "\n",
    "    dataset = Dataset.from_pandas(df_sampled)\n",
    "    raw_dataset=dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "    dataset = {}\n",
    "    for split in ['train', 'test']:\n",
    "        dataset[split] = []\n",
    "        for idx,data in enumerate(raw_dataset[split]):\n",
    "            input_example = InputExample(text_a = data[column_name[0]], label=int(data[column_name[1]]), guid=idx)\n",
    "            dataset[split].append(input_example)\n",
    "    plm, tokenizer, model_config, WrapperClass = load_plm(model_root_name,model_full_name )\n",
    "\n",
    "    template_text = '[CLS] {\"placeholder\":\"text_a\"}. Sentiment: {\"mask\"}. [SEP]'\n",
    "    mytemplate = ManualTemplate(tokenizer=tokenizer, text=template_text)\n",
    "    # wrapped_MLMTokenizerWrapper= MLMTokenizerWrapper(max_seq_length=128, decoder_max_length=3, tokenizer=tokenizer,truncate_method=\"head\")\n",
    "\n",
    "\n",
    "    train_dataloader = PromptDataLoader(dataset=dataset[\"train\"], template=mytemplate, tokenizer=tokenizer,\n",
    "        tokenizer_wrapper_class=WrapperClass, max_seq_length=256, decoder_max_length=3,\n",
    "        batch_size=4,shuffle=True, teacher_forcing=False, predict_eos_token=False,\n",
    "        truncate_method=\"head\")\n",
    "\n",
    "\n",
    "\n",
    "    # for example the verbalizer contains multiple label words in each class\n",
    "    myverbalizer = ManualVerbalizer(tokenizer, num_classes=len(class_name),\n",
    "                            label_words=class_name)\n",
    "\n",
    "\n",
    "\n",
    "    use_cuda = True\n",
    "    prompt_model = PromptForClassification(plm=plm,template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)\n",
    "    if use_cuda:\n",
    "        prompt_model=  prompt_model.cuda()\n",
    "\n",
    "    # Now the training is standard\n",
    "\n",
    "    loss_func = torch.nn.CrossEntropyLoss()\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    # it's always good practice to set no decay to biase and LayerNorm parameters\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=1e-4)\n",
    "    for epoch in range(epochs):\n",
    "        tot_loss = 0\n",
    "        for step, inputs in enumerate(train_dataloader):\n",
    "            if use_cuda:\n",
    "                inputs = inputs.cuda()\n",
    "            logits = prompt_model(inputs)\n",
    "            labels = inputs['label']\n",
    "            loss = loss_func(logits, labels)\n",
    "            loss.backward()\n",
    "            tot_loss += loss.item()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            if step %100 ==1:\n",
    "                print(\"Epoch {}, average loss: {}\".format(epoch, tot_loss/(step+1)), flush=True)\n",
    "\n",
    "    validation_dataloader = PromptDataLoader(dataset=dataset[\"test\"], template=mytemplate, tokenizer=tokenizer,\n",
    "        tokenizer_wrapper_class=WrapperClass, max_seq_length=256, decoder_max_length=3,\n",
    "        batch_size=4,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
    "        truncate_method=\"head\")\n",
    "    allpreds = []\n",
    "    alllabels = []\n",
    "    for step, inputs in enumerate(validation_dataloader):\n",
    "        if use_cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        logits = prompt_model(inputs)\n",
    "        labels = inputs['label']\n",
    "        alllabels.extend(labels.cpu().tolist())\n",
    "        allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
    "\n",
    "    acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
    "    file_path = 'result.txt'\n",
    "    string_to_write = f\"********\\nNumber of samples: {len(df)}\\nEpochs: {epochs}\\nmodel_root_name: {model_root_name}\\nmodel_full_name: {model_full_name}\\ntemplate_text: {template_text}\\nAccuracy: {acc}\\n********\\n\"\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'a') as file:\n",
    "            file.write(string_to_write)\n",
    "    else:\n",
    "        with open(file_path, 'w') as file:\n",
    "            file.write(string_to_write)\n",
    "    print(f\"acc: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_excel(r\"testing data//ABSA_Datasets.xlsx\")\n",
    "dd=[]\n",
    "xx=[dd.append([x]) for x in set(data['class_label'])]\n",
    "s=set(data['class_label'])\n",
    "class_map={}\n",
    "for idx,x in enumerate(s):\n",
    "    class_map[x]=idx\n",
    "data['label']=[class_map[x] for x in data['class_label']]\n",
    "df_shuffled = data.sample(frac=1).reset_index(drop=True)"
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
    "bert_model(\"bert\",\"google-bert/bert-base-multilingual-cased\",df_shuffled,10,[\"text\",\"label\"],dd)\n"
      "Some weights of the model checkpoint at google-bert/bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 3716it [00:01, 3382.71it/s]\n",
      "/home/sdm/anaconda3/envs/openprompt/lib/python3.8/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, average loss: 2.3964193165302277\n",
      "Epoch 0, average loss: 1.1241747824584736\n",
      "Epoch 0, average loss: 1.0555298912643205\n",
      "Epoch 0, average loss: 1.0193058980616512\n",
      "Epoch 0, average loss: 1.007579527535842\n",
      "Epoch 0, average loss: 0.9892212059868284\n",
      "Epoch 0, average loss: 0.9817483565736055\n",
      "Epoch 0, average loss: 0.9778135153133305\n",
      "Epoch 0, average loss: 0.9735018268339057\n",
      "Epoch 0, average loss: 0.9757185772623033\n",
      "Epoch 1, average loss: 1.1561328172683716\n",
      "Epoch 1, average loss: 0.9408793165987613\n",
      "Epoch 1, average loss: 0.9466251646823222\n",
      "Epoch 1, average loss: 0.9441137488512014\n",
      "Epoch 1, average loss: 0.9552526268644712\n",
      "Epoch 1, average loss: 0.9520534629009635\n",
      "Epoch 1, average loss: 0.9605830349894457\n",
      "Epoch 1, average loss: 0.956319313400831\n",
      "Epoch 1, average loss: 0.9522216087044623\n",
      "Epoch 1, average loss: 0.9513992045049922\n",
      "Epoch 2, average loss: 0.7743470370769501\n",
      "Epoch 2, average loss: 0.9340124200372135\n",
      "Epoch 2, average loss: 0.9599855760536572\n",
      "Epoch 2, average loss: 0.952954957027309\n",
      "Epoch 2, average loss: 0.9539383111605003\n",
      "Epoch 2, average loss: 0.9535912128083734\n",
      "Epoch 2, average loss: 0.9520428447034271\n",
      "Epoch 2, average loss: 0.9560272612972477\n"
     ]
    "bert_model(\"bert\",\"google-bert/bert-base-multilingual-cased\",data,10,[\"text\",\"class_label\"],dd)\n",
    "# bert_model(\"google-bert/bert-base-multilingual-cased\",4,20)\n",
    "# bert_model(\"google-bert/bert-base-multilingual-cased\",8,20)\n",
    "bert_model(\"bert\",\"google-bert/bert-base-multilingual-cased\",df_shuffled,10,[\"text\",\"label\"],dd)\n",
    "# bert_model(\"google-bert/bert-base-multilingual-cased\",32,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model(\"distilbert/distilbert-base-multilingual-cased\",4,20)\n",
    "bert_model(\"distilbert/distilbert-base-multilingual-cased\",8,20)\n",
    "bert_model(\"distilbert/distilbert-base-multilingual-cased\",16,20)\n",
    "bert_model(\"distilbert/distilbert-base-multilingual-cased\",32,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model(\"roberta-large\",4,20)\n",
    "bert_model(\"roberta-large\",8,20)\n",
    "bert_model(\"roberta-large\",16,20)\n",
    "bert_model(\"roberta-large\",32,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model(\"xlm-roberta-base\",4,20)\n",
    "bert_model(\"xlm-roberta-base\",8,20)\n",
    "bert_model(\"xlm-roberta-base\",16,20)\n",
    "bert_model(\"xlm-roberta-base\",32,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openprompt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
